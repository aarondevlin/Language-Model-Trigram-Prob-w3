{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of documents: 110\n"
     ]
    }
   ],
   "source": [
    "# Create a Bigram and Trigram Language Model from txt files of movie reviews\n",
    "# Create a function that finds the probability of the third word in a 3 word string, baed on the Trigram Lang Model\n",
    "\n",
    "# There are 55 txt files that are positive movie reviews and 55 that have negative sentiment\n",
    "\n",
    "import os, string, numpy, pandas, nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "docnamesPos = os.listdir('HW1/Pos')\n",
    "docnamesNeg = os.listdir('HW1/Neg')\n",
    "docnames = docnamesPos + docnamesNeg\n",
    "doccount = len(docnames) #110 items\n",
    "print('The number of documents:', doccount)\n",
    "#print(docnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create two DOCUMENT TOKEN Dataframes\n",
    "# rows = doc names, cols = num tokens & string of all tokens\n",
    "\n",
    "dtdfPos = pandas.DataFrame(index = docnamesPos, columns = ['#Tokens', 'Tokens'])  \n",
    "dtdfNeg = pandas.DataFrame(index = docnamesNeg, columns = ['#Tokens', 'Tokens'])\n",
    "\n",
    "frames = [dtdfPos, dtdfNeg]\n",
    "dtdfFull = pandas.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill Document Dataframes (Pos, Neg, Full) with num tokens & string of all tokens\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "alltxtPos =\"\"\n",
    "for vlogi in (docnamesPos):\n",
    "    vltxt =  open('HW1/Pos/' + vlogi, encoding='utf-8-sig').read()\n",
    "    vltxt = vltxt.lower().replace('\\ufeff', '')\n",
    "    alltxtPos += vltxt\n",
    "    doctokens = nltk.word_tokenize(vltxt.translate(str.maketrans('','',string.punctuation)))\n",
    "    filtered_sentence = []\n",
    "    \n",
    "    for w in doctokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "            \n",
    "    tokensPos = filtered_sentence       \n",
    "    dtdfPos.loc[vlogi] = pandas.Series({'#Tokens':len(filtered_sentence), 'Tokens':filtered_sentence}) # fill data\n",
    "\n",
    "alltxtNeg =\"\"\n",
    "for vlogi in (docnamesNeg):\n",
    "    vltxt =  open('HW1/Neg/' + vlogi, encoding='utf-8-sig').read()\n",
    "    vltxt = vltxt.lower().replace('\\ufeff', '')\n",
    "    alltxtNeg += vltxt                             \n",
    "    doctokens = nltk.word_tokenize(vltxt.translate(str.maketrans('','',string.punctuation)))\n",
    "    filtered_sentence = []\n",
    "   \n",
    "    for w in doctokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)    \n",
    "    \n",
    "    tokensNeg = filtered_sentence\n",
    "    dtdfNeg.loc[vlogi] = pandas.Series({'#Tokens':len(filtered_sentence), 'Tokens':filtered_sentence}) # fill data    \n",
    "\n",
    "frames = [dtdfPos, dtdfNeg]\n",
    "dtdfFull = pandas.concat(frames)\n",
    "\n",
    "alltxt = alltxtPos + alltxtNeg \n",
    "#dtdfFull.items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Word Tokens in the dataset: 14064\n"
     ]
    }
   ],
   "source": [
    "# Find total tokens (including duplicates) for full dataset\n",
    "\n",
    "alltokens = nltk.word_tokenize(alltxt.translate(str.maketrans('','',string.punctuation))) #list of all tokens in the db including repeats\n",
    "filtered_sentence = []\n",
    "   \n",
    "for w in alltokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)    \n",
    "\n",
    "alltokens = filtered_sentence\n",
    "totaltokens = len(alltokens)\n",
    "print(\"Number of Word Tokens in the dataset:\", totaltokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size (number of unique words) of the dataset: 4900\n"
     ]
    }
   ],
   "source": [
    "# Find vocabulary size (|V|, which is only uniques) for full dataset\n",
    "\n",
    "uniqs = set(alltokens)\n",
    "v = len(uniqs)\n",
    "print(\"Vocabulary size (number of unique words) of the dataset:\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Acquire Pos & Neg Bigrams & Trigrams and place into pos & neg frequency distributions \n",
    "\n",
    "# Positive\n",
    "\n",
    "trisPos = nltk.trigrams(tokensPos)\n",
    "bisPos = nltk.bigrams(tokensPos)\n",
    "\n",
    "freqdisttriPos = nltk.FreqDist(trisPos) #compute frequency distribution for Positive bis and tris in the text\n",
    "freqdistbiPos = nltk.FreqDist(bisPos)\n",
    "\n",
    "\n",
    "# Negative\n",
    "\n",
    "trisNeg = nltk.trigrams(tokensNeg)\n",
    "bisNeg = nltk.bigrams(tokensNeg)\n",
    "\n",
    "freqdisttriNeg = nltk.FreqDist(trisNeg) #compute frequency distribution for Negative bis and tris in the text\n",
    "freqdistbiNeg = nltk.FreqDist(bisNeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Positive Bigrams:\n",
      "('br', 'br') 3\n",
      "('costner', 'movie') 2\n",
      "('rescue', 'mission') 2\n",
      "('inside', 'look') 2\n",
      "('looking', 'forward') 1\n",
      "('forward', 'guardian') 1\n",
      "('guardian', 'walked') 1\n",
      "('walked', 'theater') 1\n",
      "('theater', 'wasnt') 1\n",
      "('wasnt', 'really') 1\n",
      "\n",
      "Top 10 Positive Trigrams:\n",
      "('looking', 'forward', 'guardian') 1\n",
      "('forward', 'guardian', 'walked') 1\n",
      "('guardian', 'walked', 'theater') 1\n",
      "('walked', 'theater', 'wasnt') 1\n",
      "('theater', 'wasnt', 'really') 1\n",
      "('wasnt', 'really', 'mood') 1\n",
      "('really', 'mood', 'particular') 1\n",
      "('mood', 'particular', 'time') 1\n",
      "('particular', 'time', 'kind') 1\n",
      "('time', 'kind', 'like') 1\n",
      "\n",
      "Top 10 Negative Bigrams:\n",
      "('wow', 'another') 1\n",
      "('another', 'kevin') 1\n",
      "('kevin', 'costner') 1\n",
      "('costner', 'hero') 1\n",
      "('hero', 'movie') 1\n",
      "('movie', 'postman') 1\n",
      "('postman', 'tin') 1\n",
      "('tin', 'cup') 1\n",
      "('cup', 'waterworld') 1\n",
      "('waterworld', 'bodyguard') 1\n",
      "\n",
      "Top 10 Negative Trigrams:\n",
      "('wow', 'another', 'kevin') 1\n",
      "('another', 'kevin', 'costner') 1\n",
      "('kevin', 'costner', 'hero') 1\n",
      "('costner', 'hero', 'movie') 1\n",
      "('hero', 'movie', 'postman') 1\n",
      "('movie', 'postman', 'tin') 1\n",
      "('postman', 'tin', 'cup') 1\n",
      "('tin', 'cup', 'waterworld') 1\n",
      "('cup', 'waterworld', 'bodyguard') 1\n",
      "('waterworld', 'bodyguard', 'wyatt') 1\n"
     ]
    }
   ],
   "source": [
    "# Top 10 Positive Bigrams & Trigrams\n",
    "\n",
    "print(\"Top 10 Positive Bigrams:\")\n",
    "for k,v in freqdistbiPos.most_common(10):\n",
    "    print(k,v)\n",
    "    \n",
    "print(\"\\nTop 10 Positive Trigrams:\")\n",
    "for k,v in freqdisttriPos.most_common(10):\n",
    "    print(k,v)\n",
    "\n",
    "    \n",
    "# Top 10 Negative Bigrams & Trigrams\n",
    "\n",
    "print(\"\\nTop 10 Negative Bigrams:\")\n",
    "for k,v in freqdistbiNeg.most_common(10):\n",
    "    print(k,v)\n",
    "    \n",
    "print(\"\\nTop 10 Negative Trigrams:\")\n",
    "for k,v in freqdisttriNeg.most_common(10):\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Trigrams (full dataset):\n",
      "('finney', 'tom', 'courtenay') 4\n",
      "('world', 'war', 'ii') 4\n",
      "('louis', 'gossett', 'jr') 3\n",
      "('albert', 'finney', 'tom') 3\n",
      "('production', 'king', 'lear') 3\n",
      "('director', 'peter', 'yates') 3\n",
      "('coast', 'guard', 'rescue') 3\n",
      "('brilliant', 'stage', 'actor') 2\n",
      "('kevin', 'costner', 'ashton') 2\n",
      "('costner', 'ashton', 'kutcher') 2\n",
      "\n",
      "Top 10 Bigrams (full dataset):\n",
      "('br', 'br') 23\n",
      "('coast', 'guard') 15\n",
      "('kevin', 'costner') 13\n",
      "('ashton', 'kutcher') 10\n",
      "('tom', 'courtenay') 10\n",
      "('albert', 'finney') 9\n",
      "('epatha', 'merkerson') 8\n",
      "('jacqueline', 'hyde') 8\n",
      "('darling', 'lili') 8\n",
      "('cover', 'girl') 8\n"
     ]
    }
   ],
   "source": [
    "# Create Bigram & Trigram Language Models for the entire dataset as one\n",
    "\n",
    "# Acquire Trigrams and place into a frequency distribution for the entire dataset\n",
    "\n",
    "tris = nltk.trigrams(alltokens)\n",
    "bis = nltk.bigrams(alltokens)\n",
    "\n",
    "freqdisttri = nltk.FreqDist(tris) #compute frequency distribution for Positive bis and tris in the text\n",
    "freqdistbi = nltk.FreqDist(bis)\n",
    "\n",
    "\n",
    "#Top 10 Positive Bigrams & Trigrams for the entire dataset\n",
    "\n",
    "print(\"Top 10 Trigrams (full dataset):\")\n",
    "for k,v in freqdisttri.most_common(10):\n",
    "    print(k,v)\n",
    "\n",
    "print(\"\\nTop 10 Bigrams (full dataset):\")\n",
    "for k,v in freqdistbi.most_common(10):\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fxn that takes a sequence of three words (w1,w2,w3) as input, and\n",
    "# computes the probability of third word using trigram language model:\n",
    "#\n",
    "# p(w3|w1,w2)  = ( count(w1,w2,w3) in dataset + 1 ) /\n",
    "#                   ( count(totalTrigrams) + |V| )     \n",
    "# \n",
    "# where |V| is the vocabulary size added to the denominator, \n",
    "# and 1 is added to the numerator for smoothing (to account \n",
    "# for unseen trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Total Number of Trigrams (needed for the prob equation)\n",
    "\n",
    "countTotalTrigrams = 0\n",
    "\n",
    "for k,v in freqdisttri.items():\n",
    "    countTotalTrigrams+=v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.110858280594468e-05\n",
      "7.110858280594468e-05\n",
      "0.00035554291402972336\n",
      "0.00021332574841783402\n",
      "0.00028443433122377873\n"
     ]
    }
   ],
   "source": [
    "# Probability Function to find probability of w3 occuring after w2 and w1\n",
    "\n",
    "def w3prob(w1, w2, w3):\n",
    "    countw1w2w3 = freqdisttri[w1, w2, w3]\n",
    "    prob = (countw1w2w3 + 1) / (countTotalTrigrams + v)\n",
    "    print(prob)\n",
    "\n",
    "    \n",
    "# Five example inputs for the function\n",
    "\n",
    "w3prob('the', 'many', 'other')\n",
    "w3prob('one', 'of', 'the')\n",
    "w3prob('finney', 'tom', 'courtenay')\n",
    "w3prob('brilliant', 'stage', 'actor')\n",
    "w3prob('coast', 'guard', 'rescue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
